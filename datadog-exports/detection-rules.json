{
  "description": "V-Commerce LLM Detection Rules - 5 Innovative Monitors",
  "rules": [
    {
      "name": "LLM Hallucination Detection - Invalid Product Recommendations",
      "type": "metric alert",
      "query": "avg(last_10m):avg:llm.recommendation.invalid_product_rate{env:hackathon,service:v-commerce} > 0.02",
      "message": "## ðŸš¨ LLM Hallucination Detected\n\n**Service:** {{service.name}}\n**Environment:** {{env}}\n\n### What's happening?\nThe LLM is recommending products that don't exist in our catalog. The invalid product rate has exceeded 2% over the last 10 minutes.\n\n**Current Rate:** {{value}}%\n**Threshold:** 2%\n\n### Impact\n- Direct impact on user trust\n- Potential revenue loss from bad recommendations\n- Customer confusion and frustration\n\n### Sample Investigation Steps\n1. Check RAG corpus freshness - Is the product catalog synced?\n2. Review recent prompt/system message changes\n3. Verify ProductCatalogService is returning complete data\n4. Check for model drift or context window issues\n\n### Runbook\n- Verify product catalog sync: Check last sync timestamp\n- Review LLM response samples in LLM Observability\n- Check for recent deployments to chatbotservice/shoppingassistantservice\n- Consider prompt engineering fixes if pattern persists\n\n@slack-llm-alerts @pagerduty-oncall",
      "tags": [
        "env:hackathon",
        "service:v-commerce",
        "team:llm",
        "detection_rule:hallucination",
        "severity:high",
        "category:quality"
      ],
      "options": {
        "thresholds": {
          "critical": 0.02,
          "warning": 0.01
        },
        "notify_no_data": false,
        "renotify_interval": 30,
        "timeout_h": 0,
        "escalation_message": "Hallucination rate still elevated. Escalating to LLM team lead.",
        "include_tags": true,
        "require_full_window": true,
        "new_host_delay": 300,
        "evaluation_delay": 60
      },
      "priority": 2
    },
    {
      "name": "LLM Prompt Injection / Adversarial Input Detection",
      "type": "metric alert",
      "query": "max(last_5m):max:llm.security.injection_attempt_score{env:hackathon,service:v-commerce} > 0.7",
      "message": "## ðŸ”´ SECURITY ALERT: Prompt Injection Attempt Detected\n\n**Service:** {{service.name}}\n**Environment:** {{env}}\n\n### What's happening?\nA potential prompt injection or adversarial input attempt has been detected. The injection attempt score has exceeded 0.7 (high confidence).\n\n**Injection Score:** {{value}}\n**Threshold:** 0.7\n\n### Potential Attack Vectors Detected\n- System prompt extraction attempts\n- Jailbreak attempts\n- SQL/code injection patterns in prompts\n- Instruction override attempts\n\n### Immediate Actions Required\n1. âš ï¸ Review the suspicious request in LLM Observability traces\n2. Check session ID and user context for repeat offenders\n3. Consider temporary rate limiting for the source IP/session\n4. Preserve logs for security investigation\n\n### Security Runbook\n- Extract full prompt from LLM Observability span\n- Check user session history for patterns\n- Review IP geolocation and request headers\n- If confirmed malicious: block session, notify security team\n- Document incident for security review\n\n@slack-security-alerts @pagerduty-security",
      "tags": [
        "env:hackathon",
        "service:v-commerce",
        "team:security",
        "detection_rule:injection",
        "severity:critical",
        "category:security"
      ],
      "options": {
        "thresholds": {
          "critical": 0.7,
          "warning": 0.5
        },
        "notify_no_data": false,
        "renotify_interval": 10,
        "timeout_h": 0,
        "escalation_message": "CRITICAL: Prompt injection attempts continuing. Immediate security review required.",
        "include_tags": true,
        "require_full_window": false,
        "new_host_delay": 0,
        "evaluation_delay": 0
      },
      "priority": 1
    },
    {
      "name": "LLM Cost-Per-Conversion Anomaly Detection",
      "type": "metric alert",
      "query": "avg(last_1h):avg:llm.cost_per_conversion{env:hackathon,service:v-commerce} / avg:llm.cost_per_conversion{env:hackathon,service:v-commerce}.rollup(avg, 604800) > 2",
      "message": "## ðŸ’° Cost-Per-Conversion Anomaly Detected\n\n**Service:** {{service.name}}\n**Environment:** {{env}}\n\n### What's happening?\nThe cost-per-conversion ratio has exceeded the 7-day moving average by more than 100%. This means we're spending significantly more on LLM tokens per successful checkout.\n\n**Current vs Baseline:** {{value}}x higher than 7-day average\n**Threshold:** 2x (100% increase)\n\n### Business Impact\n- Direct impact on profit margins\n- Potential sign of inefficient LLM usage\n- May indicate conversation loops or overly long prompts\n\n### Cost Breakdown Investigation\n1. Check token usage by service (chatbot vs PEAU agent vs shopping assistant)\n2. Identify most expensive conversation patterns\n3. Review prompt lengths and response sizes\n4. Check for conversation loops (user repeating questions)\n\n### Optimization Suggestions\n- Review and optimize system prompts for conciseness\n- Implement response caching for common queries\n- Consider model tier appropriateness (Gemini Flash vs Pro)\n- Evaluate context window trimming strategies\n\n### AI-Generated Insights\nCheck the Observability Insights Service for automated cost optimization recommendations.\n\n@slack-finops @slack-llm-alerts",
      "tags": [
        "env:hackathon",
        "service:v-commerce",
        "team:finops",
        "detection_rule:cost_anomaly",
        "severity:medium",
        "category:business"
      ],
      "options": {
        "thresholds": {
          "critical": 2,
          "warning": 1.5
        },
        "notify_no_data": false,
        "renotify_interval": 60,
        "timeout_h": 0,
        "escalation_message": "Cost anomaly persisting. Consider pausing non-essential LLM features.",
        "include_tags": true,
        "require_full_window": true,
        "new_host_delay": 300,
        "evaluation_delay": 300
      },
      "priority": 3
    },
    {
      "name": "LLM Response Quality Degradation Alert",
      "type": "metric alert",
      "query": "avg(last_5m):avg:llm.response.quality_score{env:hackathon,service:v-commerce} < 0.6",
      "message": "## âš ï¸ LLM Response Quality Degradation Detected\n\n**Service:** {{service.name}}\n**Environment:** {{env}}\n\n### What's happening?\nThe LLM response quality score has dropped below acceptable levels. This indicates the chatbot/assistant responses are becoming less helpful to users.\n\n**Current Quality Score:** {{value}}\n**Threshold:** 0.6 (minimum acceptable)\n\n### Quality Signals Affected\n- Response coherence and relevance\n- Product ID extraction success rate\n- Response length appropriateness\n- User engagement with recommendations\n\n### Potential Root Causes\n1. Model being rate-limited by Vertex AI\n2. Context window overflow (too much history)\n3. RAG retrieval returning irrelevant documents\n4. Recent prompt/system message changes\n5. Model endpoint health issues\n\n### Investigation Steps\n1. Check Vertex AI quotas and rate limits\n2. Review sample degraded responses in LLM Observability\n3. Verify model endpoint latency (high latency = potential throttling)\n4. Check recent deployments to LLM services\n5. Review context window usage patterns\n\n### Runbook\n- Verify Vertex AI quotas: `gcloud ai quotas list`\n- Check model endpoint health in GCP Console\n- Review recent prompt changes in git history\n- Consider fallback to cached responses if critical\n\n@slack-llm-alerts @pagerduty-oncall",
      "tags": [
        "env:hackathon",
        "service:v-commerce",
        "team:llm",
        "detection_rule:quality",
        "severity:high",
        "category:quality"
      ],
      "options": {
        "thresholds": {
          "critical": 0.6,
          "warning": 0.7
        },
        "notify_no_data": false,
        "renotify_interval": 15,
        "timeout_h": 0,
        "escalation_message": "Quality degradation persisting. Consider enabling fallback responses.",
        "include_tags": true,
        "require_full_window": true,
        "new_host_delay": 300,
        "evaluation_delay": 60
      },
      "priority": 2
    },
    {
      "name": "AI-Powered Predictive Capacity Alert",
      "type": "metric alert",
      "query": "avg(last_15m):avg:llm.prediction.error_probability{env:hackathon,service:v-commerce} > 0.8",
      "message": "## ðŸ”® Predictive Alert: Failure Predicted Within 2 Hours\n\n**Service:** {{service.name}}\n**Environment:** {{env}}\n\n### What's happening?\nThe AI-powered Observability Insights Service has predicted a high probability of failure within the next 2 hours based on current traffic patterns and system behavior.\n\n**Prediction Confidence:** {{value}}%\n**Threshold:** 80% confidence\n\n### Predicted Failure Scenarios\n- Approaching Vertex AI rate limits based on traffic trajectory\n- Latency degradation pattern similar to past incidents\n- Error rate trending upward\n- Resource utilization approaching critical levels\n\n### Proactive Actions Recommended\n1. **Pre-scale resources** if using autoscaling\n2. **Enable request queuing** to smooth traffic spikes\n3. **Warm up caches** for common queries\n4. **Alert downstream services** of potential degradation\n5. **Prepare rollback plan** if recent deployment\n\n### AI-Generated Analysis\nThe Observability Insights Service analyzed:\n- 24-hour traffic patterns\n- Current resource utilization trajectory\n- Historical incident patterns\n- Model latency trends\n\n### Similar Past Incidents\nCheck Datadog Incident Management for similar patterns.\n\n### Runbook\n- Review current vs projected traffic\n- Check scaling policies and limits\n- Verify backup/failover readiness\n- Notify stakeholders of potential impact\n\n@slack-llm-alerts @slack-sre",
      "tags": [
        "env:hackathon",
        "service:v-commerce",
        "team:sre",
        "detection_rule:predictive",
        "severity:warning",
        "category:capacity"
      ],
      "options": {
        "thresholds": {
          "critical": 0.8,
          "warning": 0.6
        },
        "notify_no_data": false,
        "renotify_interval": 30,
        "timeout_h": 2,
        "escalation_message": "Predicted failure window approaching. Escalating to SRE lead.",
        "include_tags": true,
        "require_full_window": true,
        "new_host_delay": 300,
        "evaluation_delay": 60
      },
      "priority": 2
    }
  ],
  "composite_monitors": [
    {
      "name": "LLM Health Composite - Critical Service Degradation",
      "type": "composite",
      "query": "( llm_hallucination_detection && llm_quality_degradation ) || llm_injection_detection",
      "message": "## ðŸš¨ CRITICAL: Multiple LLM Health Issues Detected\n\n**Composite Alert Triggered**\n\nMultiple detection rules are firing simultaneously, indicating a significant system issue.\n\n### Active Alerts\n- Hallucination Detection: Check for invalid product recommendations\n- Quality Degradation: Check for response quality issues\n- OR Injection Detection: Security alert triggered\n\n### Immediate Actions\n1. Check all LLM services status\n2. Review recent deployments\n3. Consider enabling maintenance mode\n4. Notify stakeholders\n\n@slack-incident-response @pagerduty-oncall",
      "tags": [
        "env:hackathon",
        "service:v-commerce",
        "team:sre",
        "detection_rule:composite",
        "severity:critical"
      ],
      "priority": 1
    }
  ],
  "metadata": {
    "version": "1.0.0",
    "created_by": "v-commerce-hackathon",
    "environment": "hackathon",
    "service": "v-commerce",
    "last_updated": "2024-12-14"
  }
}
